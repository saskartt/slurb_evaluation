{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensivivity test analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import re\n",
    "import xarray as xr\n",
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "from src.config import get_config, get_dask_cluster\n",
    "from src.plotting import SensitivityGridPlot, SensitivityTestSummaryFigure\n",
    "from src.analysis import (\n",
    "    time_filter,\n",
    "    time_to_datetimeindex,\n",
    "    map_datasets_to_experiments,\n",
    "    compute_experiment_modifications,\n",
    "    aggregate_friction_velocities,\n",
    "    compute_relative_humidities,\n",
    "    compute_shf_to_net_rad,\n",
    "    compute_response_factors,\n",
    "    aggregate_ta_2m,\n",
    "    compute_abs_diff_to_baseline,\n",
    "    bbox_filter,\n",
    ")\n",
    "from src.job_generation import read_namelist\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "config = get_config()\n",
    "cluster, client = get_dask_cluster(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define target spatial averaging area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_offset = (2110 + 2 * 896 - 1536, 1280)\n",
    "\n",
    "total_target_area_bbox = (\n",
    "    urban_offset[0] + 256,\n",
    "    urban_offset[1] + 256,\n",
    "    urban_offset[0] + 1536 - 256,\n",
    "    urban_offset[1] + 1536 - 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_target_area_filter = partial(bbox_filter, bbox=total_target_area_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daytime_filter = partial(\n",
    "    time_filter,\n",
    "    start_time=np.datetime64(\"2018-03-30T12:00\"),\n",
    "    end_time=np.datetime64(\"2018-03-30T16:00\"),\n",
    ")\n",
    "nighttime_filter = partial(\n",
    "    time_filter,\n",
    "    start_time=np.datetime64(\"2018-03-31T00:00\"),\n",
    "    end_time=np.datetime64(\"2018-03-31T04:00\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_p3d = read_namelist(Path(config.path.experiments.sensitivity) / \"base_p3d.yml\")\n",
    "origin_date_time = pd.Timestamp(\n",
    "    baseline_p3d[\"initialization_parameters\"][\"origin_date_time\"]\n",
    ")\n",
    "output_path = Path(config.path.data.jobs) / \"slurb_s_base\" / \"OUTPUT\"\n",
    "baseline_outputs = {\n",
    "    \"av_3d\": xr.open_dataset(\n",
    "        output_path / \"slurb_s_base_av_3d.000.nc\", chunks={\"time\": \"auto\"}\n",
    "    ),\n",
    "    \"av_xy\": xr.open_dataset(\n",
    "        output_path / \"slurb_s_base_av_xy.000.nc\", chunks={\"time\": \"auto\"}\n",
    "    ),\n",
    "    \"av_xz\": xr.open_dataset(\n",
    "        output_path / \"slurb_s_base_av_xz.000.nc\", chunks={\"time\": \"auto\"}\n",
    "    ),\n",
    "    \"pr\": xr.open_dataset(\n",
    "        output_path / \"slurb_s_base_pr.000.nc\", chunks={\"time\": \"auto\"}\n",
    "    ),\n",
    "    \"ts\": xr.open_dataset(\n",
    "        output_path / \"slurb_s_base_ts.000.nc\", chunks={\"time\": \"auto\"}\n",
    "    ),\n",
    "    \"xy\": xr.open_dataset(\n",
    "        output_path / \"slurb_s_base_xy.000.nc\", chunks={\"time\": \"auto\"}\n",
    "    ),\n",
    "}\n",
    "for dataset_name in baseline_outputs.keys():\n",
    "    baseline_outputs[dataset_name] = time_to_datetimeindex(\n",
    "        baseline_outputs[dataset_name], origin_date_time\n",
    "    )\n",
    "    # Offset aggregation period labels to period center\n",
    "    if len(dataset_name.split(\"_\")) > 1:\n",
    "        baseline_outputs[dataset_name][\"time\"] = baseline_outputs[dataset_name][\n",
    "            \"time\"\n",
    "        ] - pd.Timedelta(15, \"m\")\n",
    "    baseline_outputs[dataset_name][\"second_of_day\"] = (\n",
    "        baseline_outputs[dataset_name].time.dt.hour * 3600\n",
    "        + baseline_outputs[dataset_name].time.dt.minute * 60\n",
    "        + baseline_outputs[dataset_name].time.dt.second\n",
    "    )\n",
    "slurb_driver_path = Path(config.path.data.jobs) / \"slurb_s_base\" / \"INPUT\"\n",
    "baseline_slurb_driver = xr.open_dataset(\n",
    "    slurb_driver_path / \"slurb_s_base_slurb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(config.path.experiments.sensitivity) / \"experiments.yml\", \"r\") as cfile:\n",
    "    experiments = yaml.safe_load(cfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment definitions don't contain information from job names (positive and negative modification). These are added to the definitions here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [job.name for job in Path(config.path.data.jobs).iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_name in experiments.keys():\n",
    "    pattern_pos = re.compile(f\"slurb_s_{experiment_name}\\\\+.*\")\n",
    "    pattern_neg = re.compile(f\"slurb_s_{experiment_name}\\\\-.*\")\n",
    "    for job in jobs:\n",
    "        if pattern_pos.search(job):\n",
    "            experiments[experiment_name][\"job_name_positive\"] = job\n",
    "        elif pattern_neg.search(job):\n",
    "            experiments[experiment_name][\"job_name_negative\"] = job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set dataset objects for the outputs. To assist bulk computations, store these in flattened dictionary as well. These are lazy-loaded so no worries with memory consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_all = map_datasets_to_experiments(experiments, origin_date_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the absolute and relative modifications done for each experiment. First, load the baseline values. The baseline values for radiation and wind speed measurements need to be computed from the dynamic driver. We use the daily median value as a reference value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    Path(config.path.experiments.sensitivity) / \"base_slurb_driver.yml\", \"r\"\n",
    ") as cfile:\n",
    "    baseline_values = yaml.safe_load(cfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dynamic_path = (\n",
    "    Path(config.path.data.jobs) / \"slurb_s_base\" / \"INPUT\" / \"slurb_s_base_dynamic\"\n",
    ")\n",
    "baseline_dynamic = xr.open_dataset(baseline_dynamic_path)\n",
    "baseline_values[\"wspeed\"] = {}\n",
    "baseline_values[\"rad_sw_in\"] = {}\n",
    "baseline_values[\"rad_lw_in\"] = {}\n",
    "baseline_values[\"wspeed\"][\"value\"] = float(\n",
    "    np.sqrt(\n",
    "        baseline_dynamic[\"init_atmosphere_u\"].isel(z=-1).mean() ** 2\n",
    "        + baseline_dynamic[\"init_atmosphere_v\"].isel(z=-1).mean() ** 2\n",
    "    )\n",
    ")\n",
    "baseline_values[\"rad_sw_in\"][\"value\"] = float(baseline_dynamic[\"rad_sw_in\"].mean())\n",
    "baseline_values[\"rad_lw_in\"][\"value\"] = float(baseline_dynamic[\"rad_lw_in\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_slurb_path = (\n",
    "    Path(config.path.data.jobs) / \"slurb_s_base\" / \"INPUT\" / \"slurb_s_base_slurb\"\n",
    ")\n",
    "baseline_slurb = xr.open_dataset(baseline_slurb_path)\n",
    "baseline_values[\"deep_soil_temperature\"] = {}\n",
    "baseline_values[\"deep_soil_temperature\"][\"value\"] = float(\n",
    "    baseline_slurb[\"deep_soil_temperature\"].mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_experiment_modifications(experiments, baseline_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_values[\"deep_soil_temperature\"][\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_daytime = {\n",
    "    r\"shf_day\": {\n",
    "        \"symbol\": r\"H\",\n",
    "        \"group\": \"Daytime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{W}~\\mathrm{m}^{-2}\\right)}$\",\n",
    "    },\n",
    "    r\"qsws_day\": {\n",
    "        \"symbol\": r\"LE\",\n",
    "        \"group\": \"Daytime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{W}~\\mathrm{m}^{-2}\\right)}$\",\n",
    "    },\n",
    "    r\"ta_2m_day\": {\n",
    "        \"symbol\": r\"T_{\\mathrm{2m}}\",\n",
    "        \"group\": \"Daytime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{K}\\right)}$\",\n",
    "    },\n",
    "    r\"slurb_t_c_day\": {\n",
    "        \"symbol\": r\"T_{C}\",\n",
    "        \"group\": \"Daytime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{K}\\right)}$\",\n",
    "    },\n",
    "    r\"slurb_rh_can_day\": {\n",
    "        \"symbol\": r\"RH_{\\mathrm{can}}\",\n",
    "        \"group\": \"Daytime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{p.p}\\right)}$\",\n",
    "    },\n",
    "    r\"us_day\": {\n",
    "        \"symbol\": r\"u_*\",\n",
    "        \"group\": \"Daytime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{m}~\\mathrm{s}^{-1}\\right)}$\",\n",
    "    },\n",
    "}\n",
    "targets_nighttime = {\n",
    "    r\"shf_night\": {\n",
    "        \"symbol\": r\"H\",\n",
    "        \"group\": \"Nighttime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{W}~\\mathrm{m}^{-2}\\right)}$\",\n",
    "    },\n",
    "    r\"qsws_night\": {\n",
    "        \"symbol\": r\"LE\",\n",
    "        \"group\": \"Nighttime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{W}~\\mathrm{m}^{-2}\\right)}$\",\n",
    "    },\n",
    "    r\"ta_2m_night\": {\n",
    "        \"symbol\": r\"T_{\\mathrm{2m}}\",\n",
    "        \"group\": \"Nighttime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{K}\\right)}$\",\n",
    "    },\n",
    "    r\"slurb_t_c_night\": {\n",
    "        \"symbol\": r\"T_{C}\",\n",
    "        \"group\": \"Nighttime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{K}\\right)}$\",\n",
    "    },\n",
    "    r\"slurb_rh_can_night\": {\n",
    "        \"symbol\": r\"RH_{\\mathrm{can}}\",\n",
    "        \"group\": \"Nighttime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{p.p}\\right)}$\",\n",
    "    },\n",
    "    r\"us_night\": {\n",
    "        \"symbol\": r\"u_*\",\n",
    "        \"group\": \"Nighttime\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{m}~\\mathrm{s}^{-1}\\right)}$\",\n",
    "    },\n",
    "}\n",
    "targets_diurnal = {\n",
    "    r\"hysteresis_index_diurnal\": {\n",
    "        \"symbol\": r\"HI\",\n",
    "        \"group\": \"Diurnal\",\n",
    "        \"units\": r\"${}_{\\left(\\mathrm{m}~\\mathrm{s}^{-1}\\right)}$\",\n",
    "    },\n",
    "}\n",
    "targets = {**targets_daytime, **targets_nighttime}  # **targets_diurnal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute further diagnostic variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate urban and non-urban friction velocities (not done by PALM due to technical reasons).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_friction_velocities(experiments, baseline_outputs, baseline_slurb_driver)\n",
    "aggregate_ta_2m(experiments, baseline_outputs, baseline_slurb_driver)\n",
    "compute_relative_humidities(experiments, baseline_outputs)\n",
    "compute_shf_to_net_rad(experiments, baseline_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute response factors\n",
    "\n",
    "These are done for so called parameter experiements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_experiments = dict(\n",
    "    filter(\n",
    "        lambda item: item[1].get(\"subcategory\", \"\")\n",
    "        in [\"material_parameters\", \"urban_morphology\"],\n",
    "        experiments.items(),\n",
    "    )\n",
    ")\n",
    "# targets = targets_daytime\n",
    "cols = list(targets.keys())\n",
    "long_names = [exp[\"long_name\"] for exp in parameter_experiments.values()]\n",
    "index = pd.Index(long_names, name=\"Parameter\")\n",
    "cells = np.full((len(parameter_experiments), len(targets)), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_data = pd.DataFrame(cells, index=index, columns=cols)\n",
    "parameter_data.insert(0, \"ID\", parameter_experiments.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_data = parameter_data.apply(\n",
    "    compute_response_factors,\n",
    "    axis=1,\n",
    "    experiments=experiments,\n",
    "    spatial_filtering=total_target_area_filter,\n",
    "    daytime_filter=daytime_filter,\n",
    "    nighttime_filter=nighttime_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute factors for forcing experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_experiments = dict(\n",
    "    filter(\n",
    "        lambda item: item[1].get(\"subcategory\", \"\")\n",
    "        in [\"external_forcing\", \"radiation\"],\n",
    "        experiments.items(),\n",
    "    )\n",
    ")\n",
    "long_names = [exp[\"long_name\"] for exp in forcing_experiments.values()]\n",
    "index = pd.Index(long_names, name=\"Forcing\")\n",
    "cells = np.full((len(forcing_experiments), len(targets)), 0.0)\n",
    "\n",
    "forcing_data = pd.DataFrame(cells, index=index, columns=cols)\n",
    "forcing_data.insert(0, \"ID\", forcing_experiments.keys())\n",
    "forcing_data = forcing_data.apply(\n",
    "    compute_response_factors,\n",
    "    axis=1,\n",
    "    experiments=experiments,\n",
    "    spatial_filtering=total_target_area_filter,\n",
    "    daytime_filter=daytime_filter,\n",
    "    nighttime_filter=nighttime_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute simple difference for parametrisations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametrisation_experiments = dict(\n",
    "    filter(\n",
    "        lambda item: item[1].get(\"category\", \"\") in [\"namelist\"],\n",
    "        experiments.items(),\n",
    "    )\n",
    ")\n",
    "long_names = [exp[\"long_name\"] for exp in parametrisation_experiments.values()]\n",
    "index = pd.Index(long_names, name=\"Parametrisation\")\n",
    "cells = np.full((len(parametrisation_experiments), len(targets)), 0.0)\n",
    "\n",
    "parametrisation_data = pd.DataFrame(cells, index=index, columns=cols)\n",
    "parametrisation_data.insert(0, \"ID\", parametrisation_experiments.keys())\n",
    "parametrisation_data = parametrisation_data.apply(\n",
    "    compute_abs_diff_to_baseline,\n",
    "    axis=1,\n",
    "    baseline_outputs=baseline_outputs,\n",
    "    experiments=experiments,\n",
    "    spatial_filtering=total_target_area_filter,\n",
    "    daytime_filter=daytime_filter,\n",
    "    nighttime_filter=nighttime_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametrisation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute max response in orsder to harmonize colorscale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_response_factor = (\n",
    "    pd.concat([parameter_data, forcing_data, parametrisation_data])\n",
    "    .drop(\"ID\", axis=1)\n",
    "    .abs()\n",
    "    .max(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridplot = SensitivityGridPlot(\n",
    "    parameter_data.drop(\"ID\", axis=1), targets, range=max_response_factor\n",
    ")\n",
    "gridplot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridplot.fig.savefig(\"results/sensitivity_parameters.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridplot = SensitivityGridPlot(\n",
    "    forcing_data.drop(\"ID\", axis=1), targets, range=max_response_factor\n",
    ")\n",
    "gridplot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridplot.fig.savefig(\"results/sensitivity_forcing.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridplot = SensitivityGridPlot(\n",
    "    parametrisation_data.drop(\"ID\", axis=1), targets, range=max_response_factor\n",
    ")\n",
    "gridplot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridplot.fig.savefig(\"results/sensitivity_parametrisations.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example diurnal cycle\n",
    "\n",
    "Plot an example of a diurnal cycle for urban fraction experiments. Rest will will go to supplementary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = SensitivityTestSummaryFigure(\n",
    "    experiments[\"fr_urb\"], baseline_outputs, total_target_area_filter\n",
    ")\n",
    "summary.plot()\n",
    "custom_lines = [\n",
    "    plt.Line2D([0], [0], color=\"tab:red\"),\n",
    "    plt.Line2D([0], [0], color=\"tab:grey\"),\n",
    "    plt.Line2D([0], [0], color=\"tab:blue\"),\n",
    "]\n",
    "summary.fig.suptitle(\"\")\n",
    "summary.fig.legend(\n",
    "    custom_lines,\n",
    "    [\n",
    "        r\"$\\mathcal{A}_{\\mathrm{urb}}=0.9$\",\n",
    "        r\"$\\mathcal{A}_{\\mathrm{urb}}=0.8$\",\n",
    "        r\"$\\mathcal{A}_{\\mathrm{urb}}=0.7$\",\n",
    "    ],\n",
    "    loc=\"upper center\",\n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(0.55, 1.05),\n",
    ")\n",
    "summary.fig.savefig(\n",
    "    \"results/sensitivity_urban_fraction_dirunal.pdf\",\n",
    "    pad_inches=0.0,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slurb_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
